# Thanos

<div align="center">
  <img src="assets/icons/thanos.png" alt="Thanos" width="200" height="200">

**Universal AI Gateway**

*Talk to any AI provider like you're talking to Claude*

![Built with Rust](https://img.shields.io/badge/Built%20with-Rust-orange?logo=rust&style=for-the-badge)
![Multi-Provider](https://img.shields.io/badge/Providers-6+-purple?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-blue?style=for-the-badge)

[![GitHub Stars](https://img.shields.io/github/stars/ghostkellz/thanos?style=social)](https://github.com/ghostkellz/thanos)
[![Issues](https://img.shields.io/github/issues/ghostkellz/thanos)](https://github.com/ghostkellz/thanos/issues)

[Quick Start](#-quick-start) â€¢ [Features](#-features) â€¢ [API](#-api) â€¢ [Architecture](#-architecture)

</div>

---

## ğŸŒŸ What is Thanos?

**Thanos** is a universal AI gateway written in Rust that runs as a gRPC/HTTP service â€” the central model hub that all your tools plug into:

- ğŸ¤– **One gateway, many providers** - Anthropic, OpenAI, xAI, Gemini, Ollama, Omen
- ğŸ”Œ **One API surface** - gRPC + HTTP endpoints, same schema for all providers
- ğŸ¯ **Omen-aware routing** - Delegate model selection to Omen for cost/latency optimization
- ğŸ” **Streaming built-in** - Server-sent events for CLIs and editors
- ğŸ”„ **Fallback chains** - If model A fails, try B, then C
- ğŸš€ **Container-friendly** - Run as a small service beside your editor/CLI tools

### Why Thanos?

You want to:
- âœ… Add Ollama/local models but don't want to change client code
- âœ… Use multiple providers without 5 different integrations
- âœ… Have editor plugins (Neovim, VS Code) that stay dumb and fast
- âœ… Let **Omen** pick the best model for you
- âœ… Have **one stable API** your tools (zeke, zeke.nvim, Grim) can call

**With Thanos:**
- ğŸ“¡ **Clients** (zeke, nvim, CLI) stay simple, editor-focused
- ğŸ§  **Omen** picks best/cheapest model based on task
- ğŸ”§ **Thanos** handles auth, streaming, provider adapters, fallbacks

---

## âš ï¸ Security Notice

**Thanos v0.1 has NO authentication on HTTP/gRPC endpoints.** This is intentional for simplicity in trusted environments.

**Safe deployments:**
- âœ… **Localhost**: Default `0.0.0.0:9000` with firewall blocking external access
- âœ… **Private LAN**: Home/office network with trusted users only
- âœ… **VPN/Tailscale**: Private network overlay (recommended for remote access)
- âœ… **Docker**: With proper network isolation and port binding

**Unsafe deployments:**
- âŒ **Public internet**: Do NOT expose ports directly to WAN
- âŒ **Shared servers**: Other users can consume your API credits
- âŒ **Cloud VMs**: Use reverse proxy with authentication (nginx + Basic Auth)

**For production:** Use a reverse proxy (nginx, Caddy, Traefale) with authentication. See [DEPLOYMENT.md](DEPLOYMENT.md) for examples.

**Authentication middleware** (API keys, JWT) is planned for v0.2.

---

## âœ¨ Features

### Core Capabilities

- âœ… **Normalized schema** - Same request/response for all providers
- âœ… **Streaming support** - Server â†’ client tokens for CLIs/editors
- âœ… **Omen-aware routing** - Delegate model choice to Omen gateway
- âœ… **Fallback chains** - If model A fails, try B, then C
- âœ… **Provider adapters** - OpenAI, Anthropic, xAI, Gemini, Ollama
- âœ… **gRPC + HTTP** - Dual interface for low-latency and web clients
- âœ… **OAuth support** - GitHub auth (planned)
- âœ… **Container-ready** - Small service, easy to deploy

### Supported Providers

| Provider | Status | Best For | Cost |
|----------|--------|----------|------|
| ğŸ¦™ **Ollama** | âœ… | Local, private, free | Free |
| ğŸ§  **Anthropic Claude** | âœ… | Complex code, reasoning | $$$ |
| ğŸ¤– **OpenAI GPT-5** | âœ… | General purpose | $$$ |
| ğŸš€ **xAI Grok** | âœ… | Conversational, fast | $$ |
| ğŸŒ **Google Gemini** | âœ… | Multimodal | $$ |
| ğŸ”€ **Omen Gateway** | âœ… | Smart routing, optimization | Variable |

---

## ğŸš€ Quick Start

### 1. Install & Run

```bash
# Clone the repository
git clone https://github.com/ghostkellz/thanos
cd thanos

# Build and run (Rust required)
cargo build --release
./target/release/thanos

# Or run in dev mode
cargo run
```

### 2. Configure (TOML)

Create `~/.config/thanos/config.toml` or `./thanos.toml`:

```toml
[server]
bind = "0.0.0.0:8080"      # HTTP endpoint
grpc = "0.0.0.0:50051"      # gRPC endpoint

[providers.anthropic]
enabled = true
api_key = "${ANTHROPIC_API_KEY}"
model = "claude-3-7-sonnet-20250219"

[providers.openai]
enabled = true
api_key = "${OPENAI_API_KEY}"
model = "gpt-4o"

[providers.ollama]
enabled = true
endpoint = "http://localhost:11434"
model = "codellama:latest"

[providers.omen]
enabled = true
endpoint = "http://localhost:3000"

[routing]
# Delegate model selection to Omen, or specify preferred provider
strategy = "omen"  # or "preferred", "round-robin", "fallback"
fallback_chain = ["anthropic", "openai", "ollama"]
```

### 3. Call It

**HTTP Example:**

```bash
curl -X POST http://localhost:8080/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-3.7-sonnet",
    "messages": [{"role": "user", "content": "Write a quicksort in Rust"}],
    "stream": true
  }'
```

**gRPC Example (from your editor):**

```rust
// In zeke, zeke.nvim, or any Rust client
use thanos_client::ThanosClient;

let mut client = ThanosClient::connect("http://localhost:50051").await?;
let response = client.chat_completion(request).await?;
```

---

## ğŸ“¡ API

### HTTP Endpoints

```
POST /v1/chat              # Chat completion (OpenAI-compatible)
POST /v1/completions       # Text completion
GET  /v1/models            # List available models
GET  /health               # Health check
GET  /metrics              # Prometheus metrics (planned)
```

### gRPC Service

```protobuf
service ThanosService {
  rpc ChatCompletion(ChatRequest) returns (stream ChatResponse);
  rpc ListModels(Empty) returns (ModelsResponse);
  rpc Health(Empty) returns (HealthResponse);
}
```

See [API docs](docs/api.md) for full reference.

---

## ğŸ—ï¸ Architecture

```
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚       Client Layer                   â”‚
       â”‚  zeke, zeke.nvim, Grim, curl         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ gRPC / HTTP
                      â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚      Thanos (Rust AI Gateway)        â”‚
       â”‚                                      â”‚
       â”‚  â€¢ auth / token validation           â”‚
       â”‚  â€¢ model registry                    â”‚
       â”‚  â€¢ provider adapters                 â”‚
       â”‚  â€¢ streaming (SSE / gRPC streams)    â”‚
       â”‚  â€¢ fallback chains                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚     Providers / Backends          â”‚
      â”‚  OpenAI â”‚ Anthropic â”‚ xAI â”‚ ...   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Omen (Router) â”‚ â† optional: picks best model
         â”‚  cost/latency  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

**Your stable API** â†’ Thanos handles all provider complexity
**Omen** â†’ picks best model / cheapest option (optional)
**Clients** (zeke, nvim) â†’ dumb, fast, editor-first

1. **Client** sends chat request via gRPC or HTTP
2. **Thanos** checks routing strategy:
   - `"omen"` â†’ delegate to Omen for model selection
   - `"preferred"` â†’ use configured provider
   - `"fallback"` â†’ try chain until success
3. **Provider adapter** formats request, streams response
4. **Streaming** â†’ tokens flow back to client in real-time

---

## ğŸ¯ Use Cases

### In Your Editor

**Neovim** (`zeke.nvim`) â†’ calls Thanos gRPC for:
- Inline completion
- Chat window
- Code actions

**VS Code / JetBrains** (planned) â†’ calls HTTP endpoint

### In Your CLI

```bash
# zeke (Rust CLI) talks to Thanos
zeke chat "Explain this error: borrow checker issue"

# Or direct HTTP
curl http://localhost:8080/v1/chat -d '{"model": "auto", "messages": [...]}'
```

### From Your Code

```rust
// Any Rust client can use the gRPC API
use thanos_client::ThanosClient;

let mut client = ThanosClient::connect("http://localhost:50051").await?;
let response = client.chat(request).await?;
```

---

## ğŸ”Œ Editor Plugins

Thanos is a **service**, not a library. Your editor plugins talk to it over gRPC/HTTP:

### Neovim

**[zeke.nvim](https://github.com/ghostkellz/zeke.nvim)** - Lua plugin

```lua
-- lazy.nvim
{
  'ghostkellz/zeke.nvim',
  config = function()
    require('zeke').setup({
      thanos_endpoint = "http://localhost:50051"  -- gRPC
    })
  end
}
```

Features: Streaming inline completion, chat, model switching

### Grim Editor

**[thanos.grim](https://github.com/ghostkellz/thanos.grim)** - Native plugin

Features: Code actions, inline AI, multi-provider aware

### VS Code / JetBrains (Planned)

HTTP endpoint integration coming soon.

---

## ğŸ› ï¸ Development

### Building from Source

```bash
# Clone
git clone https://github.com/ghostkellz/thanos
cd thanos

# Build
cargo build --release

# Run tests
cargo test

# Run dev server
cargo run
```

### Project Structure

```
thanos/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs           # Service entry point
â”‚   â”œâ”€â”€ server.rs         # HTTP + gRPC servers
â”‚   â”œâ”€â”€ config.rs         # TOML configuration
â”‚   â”œâ”€â”€ routing.rs        # Omen-aware routing
â”‚   â”œâ”€â”€ providers/        # Provider adapters
â”‚   â”‚   â”œâ”€â”€ openai.rs
â”‚   â”‚   â”œâ”€â”€ anthropic.rs
â”‚   â”‚   â”œâ”€â”€ xai.rs
â”‚   â”‚   â”œâ”€â”€ ollama.rs
â”‚   â”‚   â””â”€â”€ omen.rs
â”‚   â””â”€â”€ proto/
â”‚       â””â”€â”€ thanos.proto  # gRPC service definition
â””â”€â”€ docs/
```

---

## ğŸ§  Why Rust Now?

The rewrite to Rust enables:

- **HTTP/3 + gRPC** - Modern protocols via `tonic`, `hyper`
- **Rich ecosystem** - Provider SDKs, OAuth, streaming, protobuf
- **Container-friendly** - Small binaries, low memory, no runtime
- **Editor-friendly streaming** - Async/await for token-by-token responses
- **Future-proof** - More crates, better tooling, easier to extend

Rust has way more production-ready libraries for multi-protocol services than Zig (for now).

---

## ğŸ¤ Contributing

Contributions welcome!

### Ways to Contribute

- ğŸ› Report bugs
- ğŸ’¡ Suggest features (more providers, auth methods, etc.)
- ğŸ”§ Submit pull requests
- ğŸ“ Improve docs
- ğŸ§ª Add tests
- â­ Star the repo!

### Development Setup

```bash
# Fork and clone
git clone https://github.com/YOUR_USERNAME/thanos
cd thanos

# Create feature branch
git checkout -b feature/amazing-feature

# Make changes, add tests
cargo test

# Commit using conventional commits
git commit -m "feat: add Gemini streaming support"

# Push and create PR
git push origin feature/amazing-feature
```

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

## ğŸ™ Credits

**Built with:**
- [Rust](https://rust-lang.org/) - Fast, safe, systems language
- [Tonic](https://github.com/hyperium/tonic) - gRPC framework
- [Hyper](https://hyper.rs/) - HTTP library
- [Tokio](https://tokio.rs/) - Async runtime

**Inspired by:**
- [Omen](https://github.com/ghostkellz/omen) - Intelligent AI routing
- [LangChain](https://github.com/langchain-ai/langchain) - LLM orchestration
- [litellm](https://github.com/BerriAI/litellm) - Multi-provider proxy

**Part of the Ghost Stack ecosystem:**
- [zeke](https://github.com/ghostkellz/zeke) - Rust CLI that calls Thanos
- [zeke.nvim](https://github.com/ghostkellz/zeke.nvim) - Neovim plugin for Thanos
- [Grim](https://github.com/ghostkellz/grim) - Editor with Thanos integration
- [Omen](https://github.com/ghostkellz/omen) - AI routing gateway

---

## ğŸ¯ Roadmap

### Core (v0.1)
- [x] Rust rewrite started
- [ ] gRPC service with provider adapters
- [ ] HTTP endpoint (OpenAI-compatible)
- [ ] Omen routing integration
- [ ] Streaming support

### Auth & Deploy (v0.2)
- [ ] GitHub OAuth
- [ ] Docker container
- [ ] Kubernetes manifests
- [ ] Prometheus metrics

### Advanced (v0.3+)
- [ ] Tool/function calling (MCP)
- [ ] Cost tracking
- [ ] Rate limiting
- [ ] Caching layer

---

## ğŸ”— Links

- **[zeke](https://github.com/ghostkellz/zeke)** - Rust CLI for Thanos
- **[zeke.nvim](https://github.com/ghostkellz/zeke.nvim)** - Neovim plugin
- **[Omen](https://github.com/ghostkellz/omen)** - AI routing service
- **[Grim](https://github.com/ghostkellz/grim)** - Editor with Thanos support

---

<div align="center">

**Made with ğŸŒŒ by the Ghost Ecosystem**

[â­ Star](https://github.com/ghostkellz/thanos) â€¢ [ğŸ“– Docs](docs/) â€¢ [ğŸ› Issues](https://github.com/ghostkellz/thanos/issues) â€¢ [ğŸ’¬ Discussions](https://github.com/ghostkellz/thanos/discussions)

</div>
